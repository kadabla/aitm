==== Agile information management
[quote, Dan McCrory]
Services and Applications can have their own Gravity, but Data is the most massive and dense, therefore it has the most gravity. Data if large enough can be virtually impossible to move.
<<McCrory2010>>

Enterprise information management (including data management) has had a contentious relationship to Agile methods. There are inherent differences in perspective between those who focus on implementing software, versus those who are concerned with data and information, especially at scale.

The tendency among software engineers is to focus on the conceptual aspects of software, without concern for the physical issues of computing. (This also is the fundamental distinction at the heart of xref:continuous-delivery[DevOps].)

But when data reaches a certain scale, its concerns start to become priorities. The bandwidth of FedEx is still greater than that of the Internet (<<Munroe2013>>). That is to say, it is more effective and efficient, past a certain scale, to physically move data on hard drives than to copy the data. The reasons for this are well understood and trace back to fundamental laws of physics and information first understood by Claude Shannon <<Shannon1949>>.

Notice that these limitations apply not just to simple movement of data, but also to more complex operations, notably the concept of xref:refactoring[refactoring].

Refactoring is one of the critical practices of Agile development, keeping systems flexible and current when used appropriately. The simple act of turning one overly-large software class or module into two or three more cohesive elements is performed every day by developers around the world, as they strive for clean, well engineered code.

But breaking an existing, overly-large database table into several more specialized tables is a different problem, if data is large. The data structure might take hours or days to be restructured, and what of the business needs in the meantime? These kinds of situations often have messy and risky solutions, that  cannot easily be "rolled back."

. A copy might need to be made for the restructuring, leaving the original table in place.
. When large restructuring operation is completed, and new code is released, a careful conversion exercise must identify the records that changed while the large restructuring occurred.
. These records must then go through the conversion process AGAIN and be updated in the new data structure. They MUST replace the older data that was initially converted in step 1.

All in all, this is an error-prone process, requiring careful auditing and cross-checking to mitigate the risk of information loss. Some system outage may be unvoidable, especially in systems with strong transactional needs for complete integrity. (See earlier discussion of xref:CAP-theorem[CAP theorem].)

Because of these issues, there will always be some contention between the software and information management perspectives. Software developers, especially those schooled in Agile practices, tend to rely on heuristics such as "do the simplest thing that could possibly work." Such a heuristic might lead to a preference for a simpler data structure, that will not hold up past a certain scale.

An information professional, when faced with the problem of restructuring the now-massive data structure, might well say "Why did you build it that way? Couldn't you have thought a little more about this?"

In fact, data management capabilities often have sought to intervene with developers, sometimes placing procedural requirements upon the development teams when database services were required. This approach is seen in waterfall development and when database teams are organized functionally. 



* The enterprise information model: a failed vision that will never go away.

BoundedContext and domain-driven design

fundamental semiotics problems - universality is not possible


* Domain driven design
* importance of patterns and ref archs - learn data through practice and pragmatism, there is no "right" way
* ontology mining
* inferred schemas
* Data as gravity
* the monolithic schema problem
* CAP revisited
* immutability: where does the data go?

http://searchcio.techtarget.com/definition/agile-business-intelligence-BI?utm_medium=EM&asrc=EM_NLN_56914325&utm_campaign=20160510_Word%20of%20the%20Day:%20Agile%20business%20intelligence%20(BI)_kherbert&utm_source=NLN&track=NL-1823&ad=907631&src=907631

* Non-invasive data governance (Seiner)

Centralized layer teams vs microservices

Microservices as a more realistic implementation mechanism, better suited to human cognition

The test data problem - nontrivial - proven to be a precursor to org performance - see Puppet Labs survey (Humble presentation 8/2016)
