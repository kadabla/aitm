==== Agile information management

===== Software versus data
[quote, Dan McCrory]
Services and Applications can have their own Gravity, but Data is the most massive and dense, therefore it has the most gravity. Data if large enough can be virtually impossible to move.
<<McCrory2010>>

Enterprise information management (including data management) has had a contentious relationship to Agile methods. There are inherent differences in perspective between those who focus on implementing software, versus those who are concerned with data and information, especially at scale.

The tendency among software engineers is to focus on the conceptual aspects of software, without concern for the physical issues of computing. (Logical versus physical is arguably the fundamental distinction at the heart of the xref:continuous-delivery[development vs operations divide].)

But when data reaches a certain scale, its concerns start to become priorities. The bandwidth of FedEx is still greater than that of the Internet (<<Munroe2013>>). That is to say, it is more effective and efficient, past a certain scale, to physically move data on hard drives than to copy the data. The reasons are well understood and trace back to fundamental laws of physics and information first understood by Claude Shannon <<Shannon1949>>. The concept of "data gravity" (quote above) seems consistent with Shannon's work.

Notice that these physical limitations apply not just to simple movement of data, but also to more complex operations, notably the concept of xref:refactoring[refactoring].

As we have previously discussed, refactoring is one of the critical practices of Agile development, keeping systems flexible and current when used appropriately. The simple act of turning one overly-large software class or module into two or three more cohesive elements is performed every day by developers around the world, as they strive for clean, well engineered code.

But breaking an existing, overly-large database table into several more specialized tables is a different problem, if data is large. Each row of data has to be processed. The data might take hours or days to be restructured, and what of the business needs in the meantime? These kinds of situations often have messy and risky solutions, that  cannot easily be "rolled back."

. A copy might need to be made for the restructuring, leaving the original table in place.
. When large restructuring operation is completed, and new code is released, a careful conversion exercise must identify the records that changed while the large restructuring occurred.
. These records must then go through the conversion process AGAIN and be updated in the new data structure. They MUST replace the older data that was initially converted in step 1.

All in all, this is an error-prone process, requiring careful auditing and cross-checking to mitigate the risk of information loss. Some system outage may be unavoidable, especially in systems with strong transactional needs for complete integrity. (See earlier discussion of xref:CAP-theorem[CAP theorem].)

Because of these issues, there will always be some contention between the software and information management perspectives. Software developers, especially those schooled in Agile practices, tend to rely on heuristics such as "do the simplest thing that could possibly work." Such a heuristic might lead to a preference for a simpler data structure, that will not hold up past a certain scale.

An information professional, when faced with the problem of restructuring the now-massive data structure, might well say "Why did you build it that way? Couldn't you have thought a little more about this?"

In fact, data management capabilities often have sought to intervene with developers, sometimes placing procedural requirements upon the development teams when database services were required. This approach is seen in waterfall development and when database teams are organized xref:product-v-function[functionally].

We saw the classic model earlier in this chapter: define

. Conceptual data model
. Logical data model
. Physical schema

as a sequential process.

But organizations pressed for time often go straight to defining physical schemas. And indeed, if the xref:cost-of-delay[Cost of Delay] is steep, this behavior will *not* change.

The only reason to invest in richer understanding of information - is if the benefits outweigh the costs. Data and records management justifies itself when:

* Systems are easier to adapt, because they are well understood
* Costs and risks of data refactoring are reduced
* Systems are easier to use, because the meaning of their information is documented for the end consumer (reducing support costs and operational risks)
* Data redundancy is lessened, saving storage and management attention
* Data and records-related risks (security, regulatory, liability) are mitigated through better data management.

Again, back to our xref:0.01-emergence[emergence model]. By the time you are an enterprise, faced with the full range of Governance, Risk, Security, and Compliance concerns, you likely need at least *some* of the benefits promised by Enterprise Information Management. But data management, like any xref:product-v-function[functional] domain, can become an end in itself, losing sight of the reasons for its existence.

===== Next generation practices in information management

====== The failure and persistence of the enterprise information model
[quote, Martin Fowler]
As you try to model a larger domain, it gets progressively harder to build a single unified model <<Fowler2014>>.

The relational database, with its fast performance and integrated schemas allowing data to be joined with great flexibility, has fundamentally defined the worldview of data managers for decades now.

This model arguably reached its peak in highly-scaled mainframe systems, where all corporate data might be available for instantaneous query and operational use.

However, when data is shared for many purposes, it becomes very difficult to change. The analysis process starts to lengthen, and xref:cost-of-delay[Cost of Delay] increases for new or enhanced systems.

This started to become a real problem right around the time that cheaper distributed systems became available in the market. Traditonal data managers of large scale mainframe database systems continued to have a perspective that "everything can fit in my database." But the demand for new digital product outstripped their capacity to analyze and incorporate the new data.

The information management landscape became fragmented. One response was the enterprise conceptual data model. The idea was that such models could represent the enterprise's information, even if it was stored in multiple databases.

However, attempting to establish such a model can run into difficulties getting agreement on definitions. (See the xref:ontology-problem[ontology problem] above.) Seeking such agreement again can impose cost of delay, if gaining agreement is required for the system. And if gaining agreement is optional, then why is agreement being sought? The risk is that the data architect becomes "ivory tower."

NOTE: In fact, there are theoretical concerns at the heart of philosophy with attempting to formulate universal ontologies. They are beyond the scope of this text but if you are interested, start by researching _semiotics_ and _postmodernism_. Such concerns may seem academic, but we see their consequences in the practical difficulty of creating universal data models.

A pragmatic response to these difficulties is represented in the Martin Fowler quote above. Fowler recommends the practice of domain-driven design, which accepts the fact that "Different groups of people will use subtly different vocabularies in different parts of a large organization" <<Fowler2014>> and quotes Eric Evans that "total unification of the domain model for a large system will not be feasible or cost-effective" <<Evans2004>>. Instead, there are various techniques for relating these contexts, beyond the scope of this book. (See <<Evans2004>>.) Some will argue for the use of microservices, but data always wants to be recombined, so microservices have limitations as a solution for the problems of information management.

And, before you completely adopt a domain-driven design approach, be certain you understand the consequences for data governance and records management. Regulators and courts will not accept "domain driven design" as a defense for non-compliance.

====== Patterns and reference architectures

Reference architectures and design patterns are examples of approaches that are known to work for solving certain problems. In other words, they are reusable (and usually free) solutions for commonly occurring scenarios. They apply to core software development, often suggesting particular class structures. <<Gamma1995>> However, the concept can also be applied to data and system architectures, e.g. <<Fowler2003>>, <<Betz2011a>>. David Hay <<Hay1996>> and Len Silverston <<Silverston2001>>, <<Silverston2001a>>, <<Silverston2008>> have documented data models for a variety of industries.

Reference architectures also can provide guidance on data structures, as they often contain industry learnings. Examples include:

[cols="3*", options="header"]
|====
|Organization|Domain|Standard(s)
|Tele-Management Forum|Telecommunications|Frameworx, ETom (Enhanced Telecommunications Operating Model), NGOSS, SIDS
|Association for Retail Technology Standards|Retail|ARTS model
|ACORD.org |Insurance|ACORD Framework
|Banking Industry Architecture Network|Banking|BIAN Service Landscape
|The Open Group Exploration, Mining, Metals and Minerals Forum|Exploration, Mining, and Minerals|Exploration and Mining Business Reference Model
|The Open Group IT4IT Forum|Information Technology Management|IT4IT Standard
|====

Patterns and reference architectures can accelerate understanding, but they also can over-complicate solutions. Understanding and applying them pragmatically is the challenge. Certainly, various well-known problems such as customer address management have surprising complexity, and can benefit from leveraging previous work.

====== Generic structures and inferred schemas

Schema development - the creation of detailed logical and physical data and/or object models - is time consuming and requires certain skills. Sometimes, application developers try to use highly generic structures in the database. Relational databases and their administrators prefer distinct tables for Customer, Invoice, and Product, with specifically identified attributes such as Invoice Date. Periodically, developers might call up the database administrator and have a conversation like this (only slightly exaggerated):

"I need some tables."

"OK, what are their descriptions?"

"Just give me 20 or so tables with 50 columns each. Call them Table1 through Table20 and Column1 through Column50. Make the columns 5000-character strings, that way they can hold anything."

"Ummm... You need to model the data. The tables and columns have to have names we can understand."

"Why? I'll have all that in the code."

These conversations usually would result in an unsatisfied developer and a DBA further convinced that developers just didn't understand data.

A relational database, for example, will not perform well at scale using such an approach. Also, there is nothing preventing the developer from mixing data in the tables, using the same columns to store different things.

This might not be a problem for smaller organizations, but in organizations with compliance requirements, knowing with confidence what data is stored where is not optional.

This does not mean that the developer was completely off track. New approaches to data warehousing use generic schemas similar to what the developer was requesting. Speed of indexing and proper records management, can be solved in a variety of ways.

Recently, the concept of the "data lake" has gained traction.

Some data has always been a challenge to adapt into traditional, rigid, structured relational databases. Modern “web-scale” companies such as Google have pioneered new, less structured data management tools and techniques.

The data lake integrates data from a large variety of sources, but does not seek to integrate them into one master structure (also known as a schema) when they are imported. Instead, the data lake requires the analysts to specify a structure when the data is extracted for analysis. This is known as "schema-on-read," in contrast to the traditional model of "schema on write."

Data lakes, and the platforms that support them (such as Hadoop) were originally created high volume web data such as generated by Google. There was no way that traditional relational databases could scale to these needs, and the data was not transactional – it was harvested and in general never updated afterwards.

This is an increasingly important kind of workload for  digital organizations. As the Internet of Things takes shape, and digital devices are embedded throughout daily experiences, high-volume, adaptable datastores (such as data lakes) will continue to spread.

Because log formats change, and the collaboration data is semi-structured, analytics will likely be better served with a “schema on read” approach. However, this means that the operational analysis is significant development. Simplifying the load logic only defers the complexity. The data lake analyst must have a thorough understanding of the various event formats and other data brought into the lake, in order to write the operational analysis query.

Schema inference at the most general shades into ontology mining. In ontology mining, data (usually text-heavy) is analyzed by algorithms to derive the data model. If one reads a textbook about the retail business, one might easily infer that there are concepts such as "store," "customer," "warehouse," and "supplier." Information technology has reached a point where such analysis itself can be automated, to a degree. Certain analytics systems have the ability to display an inferred table structure derived from unstructured or semi-structured data. This is an active area of research, development, and product innovation.

The challenge is that data still needs to be tagged and identified; the regulatory concerns do not go away. For further information and the current state of industry practice on these questions, see the professional associations at the end of this chapter.

====== Non-invasive data governance (Seiner?)


====== Test data

A non-obvious and non-trivial problem at the intersection of Enterprise Information Management and DevOps is test data management.

What is test data management?

Suppose you are a developer working on a data-intensive system, one that (for example) handles millions of customer or supply chain records.

Your code needs to support a wide variety of data inputs and outputs. At first, you just entered a few test names and addresses, like "Mickey Mouse" or "Bugs Bunny, 123 Carrot Way, Albuquerque, New Mexico 10001."

But this nonsensical data quickly 

The test data problem - nontrivial - proven to be a precursor to org performance - see Puppet Labs survey (Humble presentation 8/2016)


====== Next gen infrastructure
* Data as gravity
* CAP revisited: ultimately, uncertainty wins

* immutability: where does the data go?

http://searchcio.techtarget.com/definition/agile-business-intelligence-BI?utm_medium=EM&asrc=EM_NLN_56914325&utm_campaign=20160510_Word%20of%20the%20Day:%20Agile%20business%20intelligence%20(BI)_kherbert&utm_source=NLN&track=NL-1823&ad=907631&src=907631

Centralized layer teams vs microservices

Microservices as a more realistic implementation mechanism, better suited to human cognition
